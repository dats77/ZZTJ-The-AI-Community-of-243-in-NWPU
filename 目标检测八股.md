
# 目标检测（CV岗）

[深度学习CV岗位面试问题总结（目标检测篇）_目标检测面试-CSDN博客](https://blog.csdn.net/qq_39056987/article/details/112104199)

### **1. 介绍YOLO，并且解释一下YOLO为什么可以这么快？**

yolo是单阶段检测算法的开山之作，最初的yolov1是在图像分类网络的基础上直接进行的改进，摒弃了二阶段检测算法中的RPN操作，**直接对输入图像进行分类预测和回归**，所以它相对于二阶段的目标检测算法而言，速度非常的快，但是**精度会低很多**；但是在迭代到目前的V4、V5版本后，yolo的精度已经可以媲美甚至超过二阶段的目标检测算法，同时保持着非常快的速度，是目前工业界内最受欢迎的算法之一。**yolo的核心思想是将输入的图像经过backbone特征提取后，将的到的特征图划分为S x S的网格，物体的中心落在哪一个网格内，这个网格就负责预测该物体的置信度、类别以及坐标位置**。

首先将图片分成7×7的网格(grid cell)，每个grid cell 生成B个bounding box(x,y,h,w)（x,y中心点坐标），同时每个gird cell 生成类别概率。

先置信度过滤，再NMS过滤。

### **2. 介绍一下YOLOv3的原理？**

yolov3采用了作者自己设计的**darknet53作为主干网络**，darknet53借鉴了残差网络的思想，与resnet101、resnet152相比，在精度上差不多的同时，有着更快的速度，网络里使用了大量的残差跳层连接，并且**抛弃了pooling池化操作，直接使用步长为2的卷积来实现下采样**。在特征融合方面，为了加强小目标的检测，引入了类似于FPN的**多尺度特征融合**，特征图在经过上采样后与前面层的输出进行concat操作，浅层特征和深层特征的融合，使得yolov3在小目标的精度上有了很大的提升。**yolov3的输出分为三个部分，首先是置信度、然后是坐标信息，最后是分类信息**。在推理的时候，特征图会等分成S x S的网格，通过设置置信度阈值对格子进行筛选，如果某个格子上存在目标，那么这个格子就负责预测该物体的置信度、坐标和类别信息。

### 3、什么是NMS操作？

去除重叠框，

**（1）**将所有框的得分排序，选中最高分及其对应的框：
**（2）**遍历其余的框，如果和当前最高分框的重叠面积(IOU)大于一定阈值，我们就将框删除
**（3）**从未处理的框中继续选一个得分最高的，重复上述过程。

### 4、IOU怎么计算？

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled.png)

### **5. YOLO、SSD和Faster-RCNN的区别，他们各自的优势和不足分别是什么？**

Faster-RCNN是基于候选区域的双阶段检测器代表作，而YOLO和SSD则是单阶段检测器的代表；在速度上，单阶段的YOLO和SSD要比双阶段的Faster-RCNN的快很多，而YOLO又比SSD要快，在精度上，Faster-RCNN精度要优于单阶段的YOLO和SSD；不过这也是在前几年的情况下，目标检测发展到现在，单阶段检测器精度已经不虚双阶段，并且保持着非常快的速度，现阶段SSD和Faster-RCNN已经不更了，但是YOLO仍在飞快的发展，目前已经迭代到V4、V5，速度更快，精度更高，在COCO精度上双双破了50map，这是很多双阶段检测器都达不到的精度，而最近的Scale yolov4更是取得了55map，成功登顶榜首。当然虽然SSD和Faster-RCNN已经不更了，但是有很多他们相关的变体，同样有着不错的精度和性能，例如Cascade R-CNN、RefineDet等等。

### **6、介绍一下CenterNet的原理，它与传统的目标检测有什么不同点？**

**CenterNet**（CVPR2019）是属于**anchor-free系列**的目标检测算法的代表作之一，与它之前的目标算法相比，速度和精度都有不小的提高，尤其是和yolov3相比，在速度相同的情况下，**CenterNet精度要比yolov3高好几个点**。它的结构非常的简单，而且不需要太多了后处理，连NMS都省了，直接检测目标的中心点和大小，实现了真正的anchor-free。**CenterNet论文中用到了三个主干网络：ResNet-18、DLA-34和Hourglass-104**，实际应用中，也可以使用resnet-50等网络作为backbone；CenterNet的算法流程是：一张512*512（1x3x512x512）的图片输入到网络中，经过backbone特征提取后得到下采样32倍后的特征图（1x2048x16x16），然后再经过三层反卷积模块上采样到128128的尺寸，最后分别**送入三个head分支进行预测：分别预测物体的类别、长宽尺寸和中心点偏置。**其中推理的核心是**从headmap中提取需要的bounding box**，通过使用3*3的最大池化，检查当前热点的值是否比周围的8个临近点值都大，每个类别取100个这样的点，经过box后处理后再进行阈值筛选，得到最终的预测框。

### **7、你知道哪些边缘端部署的方案？**

- nvidia GPU：**pytorch->onnx->TensorRT**
- intel CPU： **pytorch->onnx->openvino**
- 移动端（手机、开发板等）：**pytorch->onnx->MNN、NCNN、TNN、TF-lite、Paddle-lite、RKNN等**

### **8、介绍一下yolov5**

yolov5和v4都是在v3基础上改进的，性能与v4基旗鼓相当，但是从用户的角度来说，易用性和工程性要优于v4，v5的原理可以分为四部分：输入端、backbone、Neck、输出端；

输入端：针对小目标的检测，**沿用了v4的mosaic增强**，当然这个也是v5作者在他复现的v3上的原创，**对不同的图片进行随机缩放、裁剪、排布后进行拼接**；二是**自适应锚框计算**，在v3、v4中，初始化锚框是通过对coco数据集的进行聚类得到，v5中将锚框的计算加入了训练的代码中，每次训练时，**自适应的计算不同训练集中的最佳锚框值**；

backbone：**沿用了V4的CSPDarkNet53结构**，但是在图片输入前加入了**Focus切片操作，CSP结构**实际上就是基于Densnet的思想，复制基础层的特征映射图，通过dense block发送到下一个阶段，从而将基础层的特征映射图分离出来。这样可以有效缓解梯度消失问题，支持特征传播，鼓励网络重用特征，从而减少网络参数数量。在V5中，提供了四种不同大小的网络结构：s、m、l、x，通过depth（深度）和width（宽度）两个参数控制。

Neck：采**用了SPP+PAN多尺度特征融合**，**PAN是一种自下而上的特征金字塔结构**，是在FPN的基础上进行的改进，相对于FPN有着更好的特征融合效果。

输出端：沿用了V3的head，**使用GIOU损失进行边框回归**，输出还是三个部分：置信度、边框信息、分类信息。

### **9、在你的项目中为什么选用yolov5模型而不用v4？**

v5可选的模型比较多，在速度和精度上对比v4有一定的优势，而且模型采用半精度存储，模型很小，训练和推理上都很友好。通常用s或者m版本的基本上都可以满足项目需求。

比较官方一点的回答：

使用Pytorch框架，对用户非常友好，能够方便地训练自己的数据集，相对于YOLOV4采用的Darknet框架，Pytorch框架更容易投入生产。代码易读，整合了大量的计算机视觉技术，非常有利于学习和借鉴。不仅易于配置环境，模型训练也非常快速，并且批处理推理产生实时结果。
能够直接对单个图像，批处理图像，视频甚至网络摄像头端口输入进行有效推理。
能够轻松的将Pytorch权重文件转化为安卓使用的ONXX格式，然后可以转换为OPENCV的使用格式，或者通过CoreML转化为IOS格式，直接部署到手机应用端。最后YOLO V5s高达140FPS的对象识别速度令人印象非常深刻，使用体验非常棒。

### **10、介绍yolov5中Focus模块的原理和作用**

Focus模块，将W、H信息集中到通道空间，**输入通道扩充了4倍**，作用是**可以使信息不丢失的情况下提高计算力**。具体操作为把一张图片每隔一个像素拿到一个值，类似于邻近下采样，这样我们就拿到了4张图，4张图片互补，长的差不多，但信息没有丢失，拼接起来相当于RGB模式下变为12个通道，通道多少对计算量影响不大，但图像缩小，大大减少了计算量。

以Yolov5s的结构为例，原始640×640×3的图像输入Focus结构，采用切片操作，先变成320×320×12的特征图，再经过一次32个卷积核的卷积操作，最终变成320×320×32的特征图。

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%201.png)

### 11、V4和V5中的**CSP结构，介绍一下它的原理和作用；**

主要解决了三个问题：1. 增强CNN的学习能力，能够在轻量化的同时保持着准确性；2. 降低计算成本；3. 降低内存开销。CSPNet改进了密集块和过渡层的信息流，优化了梯度反向传播的路径，提升了网络的学习能力，同时在处理速度和内存方面提升了不少。
YOLOv5s的CSP结构是将原输入分成两个分支，分别进行卷积操作使得通道数减半，然后一个分支进行Bottleneck * N操作，然后concat两个分支，使得BottlenneckCSP的输入与输出是一样的大小，这样是为了让模型学习到更多的特征。（它将feature map拆成两个部分，一部分进行卷积操作，另一部分和上一部分卷积操作的结果进行concate。）

CBL(conv+BN+Leaky relu)改成CBS(conv+BN+SiLU)

BottleneckCSP的网络结构图：

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%202.png)

C3结构图：

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%203.png)

### 12、**介绍Faster R-CNN和Cascade R-CNN**

Faster-RCNN是基于候选区域的双阶段检测器代表作，总的来说可以分为四部分：首先是**主干卷积网络的特征提取**，然后是**RPN层**，RPN层通过softmax判断anchors属于positive或者negative，再**利用边框回归修正anchors获得精确的候选区域**，RPN生成了大量的候选区域，这些候选区域和feature maps一起送入ROI pooling中，得到了候选特征区域，最后**送入分类层中进行类别判断和边框回归**，得到最终的预测结果。

Cascade R-CNN算法是在Faster R-CNN上的改进，通过**级联几个检测网络达到不断优化预测结果的目的**，预普通的级联不同，Cascade R-CNN的几个检测网络是基于不同的IOU阈值确定的正负样本上训练得到的。简单来说cascade R-CNN是由一系列的检测模型组成，**每个检测模型都基于不同IOU阈值的正负样本训练得到**，前一个检测模型的输出作为后一个检测模型的输入，因此是stage by stage的训练方式，而且越往后的检测模型，其界定正负样本的IOU阈值是不断上升的。

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%204.png)

### 13、**对于小目标检测，你有什么好的方案或者技巧？**

1. 图像金字塔和多尺度滑动窗口检测（MTCNN）
2. 多尺度特征融合检测（FPN、PAN、ASFF等）
3. 增大训练、检测图像分辨率；
4. 超分策略放大后检测；

### 14、目标检测中如何处理正负样本不平衡的问题

[一文让你轻松理解Focal Loss、正/负样本、难/易分样本 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/492396800)

**one-stage的精度不如two-stage的精度，一个主要的原因是训练过程中样本极度不均衡造成的。**

样本不均衡问题：指在训练的时候各个类别的样本数量不均衡，由于检测算法各不相同，以及数据集之间的差异，可能会存在**正负样本、难易样本、类别间样本**这3种不均衡问题。
**Focal loss：专注难样本**

### **难/易分样本**

- 首先，我们从字面意思来理解难分样本（hard example）和易分样本（easy sample）。

样本的“难分”还是“易分”，其实是模型视角下的概念。

比如说**现在有1个样本，如果模型很容易就能将其正确分类，那该样本对于模型就是易分样本；如果模型很难将其正确分类，那该样本对于模型就是难分样本**。这就像我们做数学题一样，对于我们来说会有难题也会有简单题。

BCE（交叉熵损失函数）

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%205.png)

α-balanced BCE Loss处理了正负样本不平衡的问题：

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%206.png)

Focal Loss处理了难易分样本不平衡的问题：

- **难易分样本数量不平衡**

易知，单个易分样本的损失小于单个难分样本的损失。如果易分样本的数量远远多于难分样本，则所有样本的损失可能会被大量易分样本的损失主导，导致难分样本无法得到充分学习。

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%207.png)

### **15、简单介绍一下yolox的设计原理**

[https://www.notion.so/YOLOx-2334b204c56b4a6b8c5ddffc7f1a5534?pvs=4](https://www.notion.so/YOLOx-2334b204c56b4a6b8c5ddffc7f1a5534?pvs=21)

### **16、目标检测网络中的backbone, neck, head**

**Backbone,** 译作骨干网络，主要指用于特征提取的，已在大型数据集(例如ImageNet|COCO等)上完成预训练，拥有预训练参数的卷积神经网络，例如：ResNet-50、Darknet53等;

**Head**，译作检测头，主要用于预测目标的种类和位置(bounding boxes)。在Backone和Head之间，会添加一些用于收集不同阶段中特征图的网络层，通常称为**Neck**

### 17、YOLOv6

[https://www.notion.so/YOLOv6-1f0743fd8b284e0b88decb9c0867f796?pvs=4](https://www.notion.so/YOLOv6-1f0743fd8b284e0b88decb9c0867f796?pvs=21)

### 18、YOLOv7

[https://www.notion.so/YOLOv7-6a1fbab5d5684d7bb14a265b70fcc2a6?pvs=4](https://www.notion.so/YOLOv7-6a1fbab5d5684d7bb14a265b70fcc2a6?pvs=21)

### 19、YOLOv8

[https://www.notion.so/YOLOv8-5421db9e2d724a20b3b9c9110ccb9144?pvs=4](https://www.notion.so/YOLOv8-5421db9e2d724a20b3b9c9110ccb9144?pvs=21)

### 20、YOLOv9

[https://www.notion.so/YOLOv9-a74d004089ad41c6a62b46eaa07747d3?pvs=4](https://www.notion.so/YOLOv9-a74d004089ad41c6a62b46eaa07747d3?pvs=21)

### **21、介绍一下repvgg的思想和优缺点**

[https://www.notion.so/RepVGG-7786904c7dbc48f18898ba793c1ddd5c?pvs=4](https://www.notion.so/RepVGG-7786904c7dbc48f18898ba793c1ddd5c?pvs=21)

### **22、介绍vit的原理**

[https://www.notion.so/ViT-e640ec9c6c9f4da9958221eac3f031f2?pvs=4](https://www.notion.so/ViT-e640ec9c6c9f4da9958221eac3f031f2?pvs=21)

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%208.png)

输入：多个patch（16x16），再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同。

### **23、介绍DETR的原理**

[https://www.notion.so/DETR-d-DEtection-TRansformer-b0d47c4b9c4c4fff86390a8bf026a0a9?pvs=4](https://www.notion.so/DETR-d-DEtection-TRansformer-b0d47c4b9c4c4fff86390a8bf026a0a9?pvs=21)

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%209.png)

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%2010.png)

解码器的输出，再用匈牙利匹配算法，匹配对应目标。

![Untitled](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88CV%E5%B2%97%EF%BC%89%20a885688babf8475bb59cd12ca8da1319/Untitled%2011.png)

### **24、介绍swin-tranformer的思想和原理**

[https://www.notion.so/Swim-Transformer-8cd116cc4f234cef995569fb557ccdff?pvs=4](https://www.notion.so/Swim-Transformer-8cd116cc4f234cef995569fb557ccdff?pvs=21)

### 25、ATSS的方法和改进

**基于anchor的模型与anchor-free模型的最本质区别：**正负样本定义方式。
**正负样本定义：**RetinaNet靠anchor box与GT box的IoU值来确定正负样本，而FCOS依据location在原图上是否落入GT box以及GT box尺寸大小与特征图level关系来综合确定正负样本
[https://www.notion.so/ATSS-39bdb701def24b5b9536999cf8fa590b?pvs=4](https://www.notion.so/ATSS-39bdb701def24b5b9536999cf8fa590b?pvs=21)

### 26、centernet的实现

[https://www.notion.so/centernet-7bda4925dcb34d60a5415dec31afb955?pvs=4](https://www.notion.so/centernet-7bda4925dcb34d60a5415dec31afb955?pvs=21)

### 27、fcos：一阶全卷积目标检测

基于FCN的逐像素目标检测算法，实现了无锚点（anchor-free）、无提议（proposal free）的解决方案，并且提出了中心度（Center—ness）的思想。

通过去除预先定义的锚框，FCOS完全的避免了关于锚框的复杂运算，例如训练过程中计算重叠度，而且节省了训练过程中的内存占用。更重要的是，本文**避免了和锚框有关且对最终检测结果非常敏感的所有超参数**。由于后处理只采用非极大值抑制(NMS)，所以本文提出的FCOS比以往基于锚框的一阶检测器具有更加简单的优点。

[https://www.notion.so/FCOS-3fbd60fa2e1344f5a90ce2ba7814d159?pvs=4](https://www.notion.so/FCOS-3fbd60fa2e1344f5a90ce2ba7814d159?pvs=21)
