# 深度学习基础学习贴

[深度学习面试知识点(八股文)总结 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/560482252)

[万字秋招算法岗深度学习八股文大全 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/667048896)

[Deep-Learning-Interview-Book/docs/深度学习.md at master · amusi/Deep-Learning-Interview-Book (github.com)](https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.md)

### Pytorch八股

[PyTorch面试题面经_pytorch面经-CSDN博客](https://blog.csdn.net/qq_21997625/article/details/105694092)

### RNN网络

[一文搞懂RNN（循环神经网络）基础篇 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/30844905)

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled.png)

**循环神经网络**的**隐藏层**的值s不仅仅取决于当前这次的输入x，还取决于上一次**隐藏层**的值s。**权重矩阵** W就是**隐藏层**上一次的值作为这一次的输入的权重。

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%201.png)

### BN、LN、IN、GN

[深度学习常用的 Normalization 方法：BN、LN、IN、GN-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1526775)

**Batch Normalization（Batch Norm）**：

**缺点**：在处理序列数据（如文本）时，Batch Norm可能不会表现得很好，因为序列数据通常长度不一，并且一次训练的Batch中的句子的长度可能会有很大的差异；此外，Batch Norm对于Batch大小也非常敏感。对于较小的Batch大小，Batch Norm可能会表现得不好，因为每个Batch的统计特性可能会有较大的波动。

**Layer Normalization（Layer Norm）**：

**优点**：Layer Norm是对每个样本进行归一化，因此它对Batch大小不敏感，这使得它在处理序列数据时表现得更好；另外，Layer Norm在处理不同长度的序列时也更为灵活。

**Instance Normalization（Instance Norm）**：

**优点**：Instance Norm是对每个样本的每个特征进行归一化，因此它可以捕捉到更多的细节信息。Instance Norm在某些任务，如风格迁移，中表现得很好，因为在这些任务中，细节信息很重要。

**缺点**：Instance Norm可能会过度强调细节信息，忽视了更宏观的信息。此外，Instance Norm的计算成本相比Batch Norm和Layer Norm更高。

**Group Normalization（Group Norm）**：

**优点**：Group Norm是Batch Norm和Instance Norm的折中方案，它在Batch的一个子集（即组）上进行归一化。这使得Group Norm既可以捕捉到Batch的统计特性，又可以捕捉到样本的细节信息。此外，Group Norm对Batch大小也不敏感。

**缺点**：Group Norm的性能取决于组的大小，需要通过实验来确定最优的组大小。此外，Group Norm的计算成本也比Batch Norm和Layer Norm更高。

输入的 feature map shape 记为[N, C, H, W]，其中N表示batch size，即N个样本；C表示通道数；H、W分别表示特征图的高度、宽度。

1. BN是在batch上，对N、H、W做归一化，而保留通道 C 的维度。BN对较小的batch size效果不好。BN适用于固定深度的前向神经网络，如CNN，不适用于RNN；
2. LN在通道方向上，对C、H、W归一化，主要对RNN效果明显；
3. IN在图像像素上，对H、W做归一化，用在风格化迁移；
4. GN将channel分组，然后再做归一化。

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%202.png)

BN的使用位置：全连接层或卷积操作之后，激活函数之前。

BN的作用：

（1）允许较大的学习率；

（2）减弱对初始化的强依赖性

（3）保持隐藏层中数值的均值、方差不变，让数值更稳定，为后面网络提供坚实的基础；

（4）有轻微的正则化作用（相当于给隐藏层加入噪声，类似Dropout）

BN存在的问题：

（1）每次是在一个batch上计算均值、方差，如果batch size太小，则计算的均值、方差不足以代表整个数据分布。

（2）batch size太大：会超过内存容量；需要跑更多的epoch，导致总训练时间变长；会直接固定梯度下降的方向，导致很难更新。

LN中同层神经元的输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。LN 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。

Layer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化。LN不依赖于batch size和输入sequence的长度，因此可以用于batch size为1和RNN中。LN用于RNN效果比较明显，但是在CNN上，效果不如BN。

### **Q：在transformer中为什么要用LN？**

transformer最初就是作为一个sequence to sequence模型被提出的，而一个sequence长度很可能是不确定的，layer normalization相对来说就是一个合适的归一化方法。

layer normalization作用是啥？
让中间层数据分布稳定，便于训练

原文链接：[https://blog.csdn.net/artistkeepmonkey/article/details/123551184](https://blog.csdn.net/artistkeepmonkey/article/details/123551184)

### Seq2seq模型（Encoder-Decoder）

[NLP中的RNN、Seq2Seq与attention注意力机制 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/52119092)

Encoder-Decoder模型，也可以称之为Seq2Seq模型。

在Encoder中根据输入数据生成一个语义编码C，C的获取方式有很多种，最简单的就是把Encoder中最后一个隐藏层赋值给C，也可以对最后一个隐藏状态做一个变换得到C，还可以对所有的隐藏状态做变换得到C。

Decoder，具体做法就是将C当做之前的初始状态h0输入到Decoder中，C还有一种做法是将C当做每一步的输入。

### ResNet

[你必须要知道CNN模型：ResNet - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/31852747)

退化现象：深层神经网络难以实现“恒等变换（*y*=*x*）”
退化现象让我们对非线性转换进行反思，非线性转换极大的提高了数据分类能力，但是，随着网络的深度不断的加大，我们在非线性转换方面已经走的太远，竟然无法实现线性转换。显然，在神经网络中增加线性转换分支成为很好的选择，于是，ResNet团队在ResNet模块中增加了快捷连接分支，在线性转换和非线性转换之间寻求一个平衡。

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%203.png)

ResNet直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层

.ResNet的一个重要设计原则是：当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。

当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4。

### 多层感知机（MLP）、FNN、FCNN、DNN

[机器学习（4）多层感知机（MLP） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/158976635)

**多层感知器(Multi-Layer Perceptron,*MLP*)也叫人工神经网络(Artificial Neural Network,ANN),除了输入输出层,它中间可以有多个隐层。**

多层感知机（MLP）是一种前向结构的人工神经网络，包含输入层、输出层及多个隐藏层，3层感知机的神经网络图如下所示：

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%204.png)

一、多层感知机（MLP）

多层感知机（MLP）是一种前馈

[神经网络](https://cloud.baidu.com/product/wenxinworkshop)

，由输入层、隐藏层和输出层组成。每个神经元接收上一层神经元的输出，并将其加权求和后经过一个非线性激活函数进行处理。MLP能够处理非线性问题，通过学习合适的权重和偏差来建立输入与输出的映射关系，被广泛用于

[机器学习](https://cloud.baidu.com/product/ai_bml.html)

和

[深度学习](https://cloud.baidu.com/product/wenxinworkshop)

任务。

二、全连接神经网络（FCNN）

全连接神经网络（FCNN）也被称为多层感知器（MLP），是一种最基础的人工神经网络结构。在全连接神经网络中，每个神经元都与前一层和后一层的所有神经元相连接，形成一个密集的连接结构。全连接神经网络能够学习输入数据的复杂特征，并进行分类、回归等任务。

三、前馈神经网络（FNN）

前馈神经网络（FNN）是人工神经网络的一种，采用一种单向多层结构。其中每一层包含若干个神经元。在此种神经网络中，各神经元可以接收前一层神经元的信号，并产生输出到下一层。第0层叫输入层，最后一层叫输出层，其他中间层叫做隐含层（或隐藏层、隐层）。隐层可以是一层，也可以是多层。整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。

四、深度神经网络（DNN）

深度神经网络（DNN）是一种具备至少一个隐藏层的神经网络，利用激活函数去线性化，使用交叉熵作损失函数，利用反向传播优化算法进行学习训练的前馈神经网络。深度神经网络能够通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

五、BP算法

BP算法是由学习过程由信号的正向传播与误差的反向传播两个过程组成。由于多层前馈网络的训练经常采用误差反向传播算法，人们也常把将多层前馈网络直接称为BP网络。正向传播时，输入样本从输入层进入网络，经隐层逐层传递至输出层，如果输出层的实际输出与期望输出(导师信号)不同，则转至误差反向传播；如果输出层的实际输出与期望输出(导师信号)相同，结束学习算法。反向传播时，将输出误差(期望输出与实际输出之差)按原通路反传计算，通过隐层反向，直至输入层，在反传过程中将误差分摊给各层的各个单元，获得各层各单元的误差信号，并将其作为修正各单元权值的根据。这一计算过程使用梯度下降法完成，在不停地调整各层神经元的权值和阈值后，使误差信号减小到最低限度。

### CNN中的归纳偏置（Inductive Biases）

机器学习中的**no-free-lunch定理**表明对于任意函数而言，一定的偏好（或者说**归纳偏置**）是实现泛化的必要手段，也就是说，没有完全通用的学习算法，**任何学习算法都只能在特定的一些分布上实现泛化。**
归纳偏置其实就是一种[先验知识](https://so.csdn.net/so/search?q=%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86&spm=1001.2101.3001.7020)（提前验证的知识），一种提前做好的假设。

在CNN中的归纳偏置一般包括两类：**①locality(局部性)和②translation equivariance（平移等变性）**

①  locality:假设相同的区域会有相同的特征，靠得越近的东西相关性能也就越强。局部性可以控制模型的复杂度。

②translation equivariance：由于卷积核是一样的所以不管图片中的物体移动到哪里，**只要是同样的输入进来遇到同样的卷积核，那么输出就是一样的**。利用平移等变形可以很好的提高模型的泛化能力。

总结：但是使用基于CNN的方法还是存在**感受野有限的问题**，不能很好的建模长远的依赖关系（**全局信息不足**），而基于transformer的方法可以很好的建模全局信息但是transformer反而缺乏类似于CNN的归纳偏置，这些先验信息必须通过大量的数据来进行学习，所以小的数据在CNN上取得的效果一般优于基于transformer的方法。训练基于CNN的方法通常只需要一个较小的数据集，而**训练基于transformer的方法一般需要再大的数据集上进行预训练**。

### 为什么CNN是局部，Transformer是全局？

Transformer模型采用自注意力机制（Self-Attention）来建模输入序列中任意两个位置之间的依赖关系，这使得模型能够同时关注输入序列中的所有位置，而不仅限于局部范围。

Transformer中的自注意力机制允许每个位置的词汇在计算编码表示时，考虑到所有其他位置的词汇，并根据它们之间的关联性来调整权重。这种机制使得模型在计算每个位置的编码表示时能够利用整个输入序列的信息，从而实现了全局归纳建模能力。

CNN中每个卷积核只关注输入数据的一个局部区域，而不是全部输入数据，这样可以有效地提取输入数据中的局部特征。卷积操作通过共享权重参数的方式，使得模型具有一定的平移不变性，即模型可以学习到某种特征不受位置变化的影响。

CNN在处理数据时会逐渐**通过多层卷积操作扩大感受野范围**，从而逐渐捕捉到输入数据的全局信息。虽然卷积神经网络在局部区域内进行特征提取，但通过**叠加多层卷积层和池化层**，可以逐渐扩大模型对输入数据的整体理解能力，实现对全局信息的建模。

### 感受野计算（**Receptive field** ）

在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在**原始图像**上映射的区域大小。

(N-1)_RF = f(N_RF, stride, kernel) = (N_RF - 1) * stride + kernel

其中，RF是感受野。N_RF和RF有点像，**N代表 neighbour**，指的是第n层的 a feature在n-1层的RF，**记住N_RF只是一个中间变量**，不要和**RF**混淆。 stride是步长，ksize是卷积核大小

### 激活函数Relu、SiLu等

Relu优点：（1）relu函数在大于0的部分梯度为常数，所以不会产生**梯度弥散现象**。而对于sigmod函数，在正负饱和区的梯度都接近于0，可能会导致梯度消失现象。（2）Relu函数的导数计算更快，所以使用梯度下降时比Sigmod收敛起来要快很多。

Relu缺点：Relu死亡问题。当 x 是小于 0 的时候，那么从此所以流过这个神经元的梯度将都变成 0；这个时候这个 ReLU 单元在训练中将死亡（也就是参数无法更新），这也导致了数据多样化的丢失（因为数据一旦使得梯度为 0，也就说明这些数据已不起作用）。

Sigmoid优点：具有很好的解释性，将线性函数的组合输出为0，1之间的概率。

Sigmoid缺点：（1）激活函数计算量大，反向传播求梯度时，求导涉及除法。（2）反向传播时，在饱和区两边导数容易为0，即容易出现梯度消失的情况，从而无法完成深层网络的训练。

**e. softmax和sigmoid在多分类任务中的优劣**

多个sigmoid与一个softmax都可以进行多分类.如果多个类别之间是互斥的，就应该使用softmax，即这个东西只可能是几个类别中的一种。如果多个类别之间不是互斥的，使用多个sigmoid。

SiLU是Sigmoid和ReLU的改进版。SiLU具备无上界有下界、平滑、非单调的特性。SiLU在深层模型上的效果优于 ReLU。可以看做是平滑的ReLU激活函数。

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%205.png)

### **如何处理样本不均衡问题**

**a. 样本的过采样和欠采样**

**欠采样** ：随机删除观测数量足够多的类，使得两个类别间的相对比例是显著的。虽然这种方法使用起来非常简单，但很有可能被我们删除了的数据包含着预测类的重要信息。

**过采样** ：对于不平衡的类别，我们使用拷贝现有样本的方法随机增加观测数量。理想情况下这种方法给了我们足够的样本数，但过采样可能导致过拟合训练数据。

**过采样和欠采样结合**

**合成采样（ SMOTE** ）：

1. 基于距离度量的方式计算两个或者多个稀有样本之间的相似性

2. 选择其中一个样本作为基础样本

3. 再在邻居样本中随机选取一定数量的样本对那个基础样本的一个属性进行噪声，每次处理一个属性。通过这样的方式产生新数据。

**b. 使用多分类器进行分类**

方法一：模型融合 （bagging的思想 ）

思路：从丰富类样本中随机的选取（有放回的选取）和稀有类等量样本的数据。和稀有类样本组合成新的训练集。这样我们就产生了多个训练集，并且是互相独立的，然后训练得到多个分类器。

若是分类问题，就把多个分类器投票的结果（少数服从多数）作为分类结果。

若是回归问题，就将均值作为最后结果。

方法二：增量模型 （boosting的思想）

思路：使用全部的样本作为训练集，得到分类器L1

从L1正确分类的样本中和错误分类的样本中各抽取50%的数据，即循环的一边采样一个。此时训练样本是平衡的。训练得到的分类器作为L2.

从L1和L2分类结果中，选取结果不一致的样本作为训练集得到分类器L3.

最后投票L1,L2,L3结果得到最后的分类结果。

**c. 将二分类问题转成其他问题**

可以将不平衡的二分类问题转换成异常点检测，或者一分类问题（可使用one-class svm建模）

**d.改变正负样本在模型中的权重**

使用代价函数学习得到每个类的权值，大类的权值小，小类的权值大。刚开始，可以设置每个类别的权值与样本个数比例的倒数，然后可以使用过采样进行调优。

**不平衡类别会造成问题有两个主要原因**：

1.对于不平衡类别，我们不能得到实时的最优结果，因为模型/算法从来没有充分地考察隐含类。

2.它对验证和测试样本的获取造成了一个问题，因为在一些类观测极少的情况下，很难在类中有代表性。

**不平衡问题的评价指标**

准确度这个评价指标在类别不均衡的分类任务中并不能work。几个比传统的准确度更有效的评价指标：

**混淆矩阵(Confusion Matrix)**：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。

### **数据增强方法**

**几何变换：**翻转，旋转，裁剪，变形，缩放

**颜色变换：**噪声、模糊、颜色变换、擦除、填充

**其他：**SMOTE，SamplePairing，mixup，mixup，上下采样，增加不同惩罚

### 过拟合的解决办法

**什么是过拟合**

过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。

**产生过拟合根本原因:**

观察值与真实值存在偏差, 训练数据不足，数据太少，导致无法描述问题的真实分布, 数据有噪声, 训练模型过度，导致模型非常复杂

**什么是欠拟合**：训练的模型在训练集上面的表现很差，在验证集上面的表现也很差

**原因**：训练的模型太简单，最通用的特征模型都没有学习到

**解决办法：**

1、正则化

2、剪枝处理

3、提前终止迭代（Early stopping）

4、权值共享

5、增加噪声

6、Batch Normalization

7、Bagging和Boosting

8、Dropout

### **正则化**

**正则化的原理**：在损失函数上加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性。

### mAP

**Precision-Recall**曲线基础上，通过计算每一个recall值对应的Precision值的平均值，可以获得一个数值形式(numerical metric)的评估指标：**AP**(Average Precision），用于衡量的是训练出来的模型在感兴趣的类别上的检测能力的好坏。

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%206.png)

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%207.png)

### **梯度爆炸，梯度消失，梯度弥散是什么，为什么会出现这种情况以及处理办法**

### **Transformer中的positional encoding**

- **为什么需要PE**: 因为transfomer是同时处理所有输入的，失去了位置信息。
- **编码应该满足的条件**：a、对于每个位置词语，编码是唯一的 b、词语之间的间隔对于不同长度句子是一致的 c、能适应任意长度句子
- **公式**：每个词语位置编码为不同频率的余弦函数，从1到1/10000。如下将每个词语位置编码为d维向量-->

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%208.png)

**可理解为一种二进制编码，二进制的不同位变化频率不一样**，PE的不同位置变化频率也不一样

### **求似然函数步骤**

- 定义：概率是给定参数，求某个事件发生概率；似然则是给定已发生的事件，估计参数。
1. 写出似然函数
2. 对似然函数取对数并整理
3. 求导数，导数为0处为最佳参数
4. 解似然方程

### **空洞卷积实现**

相比于常规卷积多了dilation rate超参数，例如dilation rate=2代表相邻两个卷积点距离为2，如图(b)。

![Untitled](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%B4%B4%200a4190cd84a5469c991d1873bd9c6e1b/Untitled%209.png)

**存在问题**：gridding effect, 由于卷积的像素本质上是采样得到的，所以图像的局部相关性丢失了，同时远距离卷积得到的信息也没有相关性。
